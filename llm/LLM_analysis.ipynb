{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa3d59d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfc91a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a16_ratio = 2\n",
    "w8a16_ratio =  1.\n",
    "w4a16_ratio = 0.5469"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2364a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddr_util = 1.\n",
    "mac_util = 0.5 # 目前一般能达到50%的利用率\n",
    "p2p_util = 1.\n",
    "pcie_avg_ms = 0.1 # ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d79876ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_embedding_weights(hidden_size, vocab_size):\n",
    "    num_weights = hidden_size * vocab_size\n",
    "    return num_weights\n",
    "\n",
    "def compute_lm_head_flops(hidden_size, vocab_size):\n",
    "    flops = hidden_size * vocab_size * 2\n",
    "    return flops\n",
    "\n",
    "def count_lm_head_weights(hidden_size, vocab_size):\n",
    "    num_weights = hidden_size * vocab_size\n",
    "    return num_weights\n",
    "    \n",
    "\n",
    "def compute_block_flops(query_len, key_len, hidden_size, inter_size, num_attn_heads, num_kv_heads):\n",
    "    num_qo_mm = 2\n",
    "    num_kv_mm = 2\n",
    "    num_attn_mm = 2\n",
    "    num_mlp_mm = 3\n",
    "    \n",
    "    head_dim = hidden_size / num_attn_heads\n",
    "    kv_dim = head_dim * num_kv_heads\n",
    "    \n",
    "    kv_flops = query_len * hidden_size * kv_dim * 2 * num_kv_mm\n",
    "    qo_flops = query_len * hidden_size * hidden_size * 2 * num_qo_mm\n",
    "    attn_flops = query_len * key_len * hidden_size * 2 * num_attn_mm\n",
    "    mlp_flops = query_len * hidden_size * inter_size * 2 * num_mlp_mm\n",
    "    \n",
    "    total_flops = kv_flops + qo_flops + attn_flops + mlp_flops\n",
    "    return total_flops\n",
    "\n",
    "def count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads):\n",
    "    num_qo_mm = 2\n",
    "    num_kv_mm = 2\n",
    "    num_mlp_mm = 3\n",
    "    \n",
    "    head_dim = hidden_size / num_attn_heads\n",
    "    kv_dim = head_dim * num_kv_heads\n",
    "    \n",
    "    num_qo_weights = hidden_size * hidden_size * num_qo_mm\n",
    "    num_kv_weights = hidden_size * kv_dim * num_kv_mm\n",
    "    num_mlp_weights = hidden_size * inter_size * num_mlp_mm\n",
    "    \n",
    "    num_weights = num_qo_weights + num_kv_weights + num_mlp_weights\n",
    "    return num_weights\n",
    "\n",
    "def count_block_gather_weights(max_seq_len, hidden_size, num_attn_heads):\n",
    "    head_dim = hidden_size / num_attn_heads\n",
    "    num_weights = max_seq_len * head_dim * 2 # cos, sin table\n",
    "    return num_weights\n",
    "\n",
    "def count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads):\n",
    "    head_dim = hidden_size / num_attn_heads\n",
    "    kv_dim = head_dim * num_kv_heads\n",
    "    \n",
    "    num_kv_cache = 2 * seq_len * kv_dim * num_layers\n",
    "    return num_kv_cache\n",
    "\n",
    "def get_prefill_compute_time(total_flops, num_device, tpu_freq=1000):\n",
    "    tpu_peak_flops = 16384 * num_device * tpu_freq / 1000\n",
    "    \n",
    "    compute_time = total_flops / 1e9 / tpu_peak_flops / mac_util\n",
    "    return compute_time\n",
    "\n",
    "def get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq=1000):\n",
    "    p2p_bw = p2p_speed\n",
    "    s2l_bw = s2l_speed * tpu_freq / 1000\n",
    "    l2s_bw = l2s_speed * tpu_freq / 1000\n",
    "    bf16_size = 2\n",
    "    ring_data_ratio = (num_device - 1) * 2 / num_device\n",
    "    size = seq_len * hidden_size * bf16_size * ring_data_ratio * num_layers * 2\n",
    "    \n",
    "    p2p_time = size / p2p_bw / p2p_util\n",
    "    add_time = (size * 2 / s2l_bw + size / l2s_bw) / ddr_util\n",
    "    allreduce_time = p2p_time + add_time\n",
    "    return allreduce_time\n",
    "\n",
    "def get_decode_pcie_time(num_layers):\n",
    "    pcie_ms = pcie_avg_ms * num_layers * 2\n",
    "    return pcie_ms\n",
    "\n",
    "def get_decode_allreduce_time(num_layers, num_device):\n",
    "    time_per_allreduce = 0\n",
    "    if num_device == 2:\n",
    "        time_per_allreduce = 0.12\n",
    "    elif num_device == 4:\n",
    "        time_per_allreduce = 0.15\n",
    "    elif num_device == 6:\n",
    "        time_per_allreduce = 0.36 # old allreduce\n",
    "    elif num_device == 8:\n",
    "        time_per_allreduce = 0.2\n",
    "    elif num_device == 1:\n",
    "        return 0\n",
    "    else:\n",
    "        print(f\"****** Do Not Support num_device = {num_device} **********\")\n",
    "        return -1\n",
    "    \n",
    "    allreduce_ms = time_per_allreduce * num_layers * 2\n",
    "    return allreduce_ms\n",
    "\n",
    "def get_decode_load_weights_time(weight_bytes, num_device, tpu_freq=1000):\n",
    "    s2l_bw = s2l_speed * tpu_freq / 1000\n",
    "    \n",
    "    load_weights_ms = weight_bytes / s2l_bw / num_device * 1000 / ddr_util\n",
    "    return load_weights_ms\n",
    "\n",
    "def get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq=1000):\n",
    "    s2l_bw = s2l_speed * tpu_freq / 1000\n",
    "    \n",
    "    load_kv_cache_ms = kv_cache_bytes / s2l_bw / num_device * 1000 / ddr_util\n",
    "    return load_kv_cache_ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bf9f2a",
   "metadata": {},
   "source": [
    "# Qwen-72B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e0191",
   "metadata": {},
   "source": [
    "## W4A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a7a412a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'p2p_speed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e61ee60cba0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprefill_compute_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prefill_compute_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_flops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprefill_allreduce_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prefill_allreduce_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdecode_pcie_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_decode_pcie_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdecode_allreduce_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_decode_allreduce_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-607e46561dd5>\u001b[0m in \u001b[0;36mget_prefill_allreduce_time\u001b[0;34m(seq_len, hidden_size, num_layers, num_device, tpu_freq)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_prefill_allreduce_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpu_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mp2p_bw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp2p_speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0ms2l_bw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2l_speed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtpu_freq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0ml2s_bw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml2s_speed\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtpu_freq\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'p2p_speed' is not defined"
     ]
    }
   ],
   "source": [
    "seq_len = 8192\n",
    "hidden_size = 8192\n",
    "inter_size = 24576\n",
    "num_attn_heads = 64\n",
    "num_kv_heads = 64\n",
    "num_layers = 80\n",
    "vocab_size = 152064\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen-72B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d4c95",
   "metadata": {},
   "source": [
    "# Qwen-14B\n",
    "注意：Qwen-14B的inter_size不要直接用config.json文件里的intermediate_size，这个size并不是Block中的MLP的inter_size，所以inter_size最好是直接看一下Block中的MLP的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0475834",
   "metadata": {},
   "source": [
    "## W4A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a66f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Qwen-14B-W4A16-seq2048-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 1485.0 MiB\n",
      "block_gather: 1.0 MiB\n",
      "lm_head: 1.55713536 GFLOPs, 406.07325000000003 MiB\n",
      "blocks: 1377.07388928 GFLOPs, 164.41181250000002 MiB\n",
      "all blocks: 55082.9555712 GFLOPs, 6576.472500000001 MiB\n",
      "mm_weights: 6982.545750000001 MiB\n",
      "total flops: 55084.51270656 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 2.125682113015873 s\n",
      "decode_time = 45.42726545798095 ms, Speed = 22.013211447318444 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2048\n",
    "hidden_size = 5120\n",
    "inter_size = 13696\n",
    "num_attn_heads = 40\n",
    "num_kv_heads = 40\n",
    "num_layers = 40\n",
    "vocab_size = 152064\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen-14B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e195b89c",
   "metadata": {},
   "source": [
    "## W8A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c5487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Qwen-14B-W8A16-seq2048-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 1485.0 MiB\n",
      "block_gather: 1.0 MiB\n",
      "lm_head: 1.55713536 GFLOPs, 742.5 MiB\n",
      "blocks: 1377.07388928 GFLOPs, 300.625 MiB\n",
      "all blocks: 55082.9555712 GFLOPs, 12025.0 MiB\n",
      "mm_weights: 12767.5 MiB\n",
      "total flops: 55084.51270656 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 2.125682113015873 s\n",
      "decode_time = 59.87003733333333 ms, Speed = 16.7028457729596 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 2048\n",
    "hidden_size = 5120\n",
    "inter_size = 13696\n",
    "num_heads = 40\n",
    "num_layers = 40\n",
    "vocab_size = 152064\n",
    "ratio = w8a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen-14B-W8A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e576fe73",
   "metadata": {},
   "source": [
    "# Llama2-13B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f457520",
   "metadata": {},
   "source": [
    "## W4A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Llama2-13B-W4A16-seq512-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 312.5 MiB\n",
      "block_gather: 0.25 MiB\n",
      "lm_head: 0.32768 GFLOPs, 85.45312500000001 MiB\n",
      "blocks: 330.17561088 GFLOPs, 165.43725 MiB\n",
      "all blocks: 13207.0244352 GFLOPs, 6617.49 MiB\n",
      "mm_weights: 6702.943125 MiB\n",
      "total flops: 13207.3521152 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 0.521589053968254 s\n",
      "decode_time = 41.733275452952384 ms, Speed = 23.961694574568934 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "hidden_size = 5120\n",
    "inter_size = 13824\n",
    "num_attn_heads = 40\n",
    "num_kv_heads = 40\n",
    "num_layers = 40\n",
    "vocab_size = 32000\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Llama2-13B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce77b3a1",
   "metadata": {},
   "source": [
    "## W8A16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12ee484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Llama2-13B-W8A16-seq512-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 312.5 MiB\n",
      "block_gather: 0.25 MiB\n",
      "lm_head: 0.32768 GFLOPs, 156.25 MiB\n",
      "blocks: 330.17561088 GFLOPs, 302.5 MiB\n",
      "all blocks: 13207.0244352 GFLOPs, 12100.0 MiB\n",
      "mm_weights: 12256.25 MiB\n",
      "total flops: 13207.3521152 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 0.521589053968254 s\n",
      "decode_time = 55.59771428571429 ms, Speed = 17.9863509291235 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "hidden_size = 5120\n",
    "inter_size = 13824\n",
    "num_attn_heads = 40\n",
    "num_kv_heads = 40\n",
    "num_layers = 40\n",
    "vocab_size = 32000\n",
    "ratio = w8a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Llama2-13B-W8A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e49a87",
   "metadata": {},
   "source": [
    "# Qwen-7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f5d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Qwen-7B-W4A16-seq8192-1dev-950MHz:\n",
      "### Basic Information\n",
      "embedding: 1187.0 MiB\n",
      "block_gather: 4.0 MiB\n",
      "lm_head: 1.244659712 GFLOPs, 324.58515000000006 MiB\n",
      "blocks: 4415.226380288 GFLOPs, 105.55170000000001 MiB\n",
      "all blocks: 141287.244169216 GFLOPs, 3377.6544000000004 MiB\n",
      "mm_weights: 3702.2395500000002 MiB\n",
      "total flops: 141288.488828928 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 18.154873667368424 s\n",
      "decode_time = 149.8569620066807 ms, Speed = 6.673029978783498 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 8192\n",
    "hidden_size = 4096\n",
    "inter_size = 11008\n",
    "num_attn_heads = 32\n",
    "num_kv_heads = 32\n",
    "num_layers = 32\n",
    "vocab_size = 151936\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 950\n",
    "num_device = 1\n",
    "\n",
    "p2p_speed = 3e9\n",
    "s2l_speed = 60e9\n",
    "l2s_speed = 45e9\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen-7B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bbd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8192\n",
    "hidden_size = 3584\n",
    "inter_size = 18944\n",
    "num_attn_heads = 28\n",
    "num_kv_heads = 2\n",
    "num_layers = 28\n",
    "vocab_size = 151936\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "p2p_speed = 3e9\n",
    "s2l_speed = 60e9\n",
    "l2s_speed = 45e9\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen-7B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637b17f5",
   "metadata": {},
   "source": [
    "# Qwen1.5-32B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ef33fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Qwen1.5-32B-W4A16-seq8192-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 1485.0 MiB\n",
      "block_gather: 4.0 MiB\n",
      "lm_head: 1.55713536 GFLOPs, 406.07325000000003 MiB\n",
      "blocks: 9298.60419584 GFLOPs, 252.25762500000002 MiB\n",
      "all blocks: 595110.66853376 GFLOPs, 16144.488000000001 MiB\n",
      "mm_weights: 16550.561250000002 MiB\n",
      "total flops: 595112.22566912 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 17.834474067301585 s\n",
      "decode_time = 84.83334514590476 ms, Speed = 11.787817612049853 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 8192\n",
    "hidden_size = 5120\n",
    "inter_size = 27392\n",
    "num_attn_heads = 40\n",
    "num_kv_heads = 8\n",
    "num_layers = 64\n",
    "vocab_size = 152064\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Qwen1.5-32B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aae4774",
   "metadata": {},
   "source": [
    "# Yi-34B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b07ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Yi-34B-W4A16-seq512-8dev-875MHz:\n",
      "### Basic Information\n",
      "embedding: 875.0 MiB\n",
      "block_gather: 0.25 MiB\n",
      "lm_head: 0.917504 GFLOPs, 239.26875 MiB\n",
      "blocks: 578.746843136 GFLOPs, 290.9508 MiB\n",
      "all blocks: 34724.81058816 GFLOPs, 17457.048000000003 MiB\n",
      "mm_weights: 17696.31675 MiB\n",
      "total flops: 34725.72809216 GFLOPs\n",
      "### Time Information\n",
      "prefill_time = 1.2172379733333334 s\n",
      "decode_time = 80.48038607725715 ms, Speed = 12.425387709249431 token/s\n"
     ]
    }
   ],
   "source": [
    "seq_len = 512\n",
    "hidden_size = 7168\n",
    "inter_size = 20480\n",
    "num_attn_heads = 56\n",
    "num_kv_heads = 8\n",
    "num_layers = 60\n",
    "vocab_size = 64000\n",
    "ratio = w4a16_ratio\n",
    "\n",
    "tpu_freq = 875\n",
    "num_device = 8\n",
    "\n",
    "lm_head_flops = compute_lm_head_flops(hidden_size, vocab_size)\n",
    "block_flops = compute_block_flops(seq_len, seq_len, hidden_size, inter_size, num_attn_heads, num_kv_heads)\n",
    "\n",
    "all_block_flops = block_flops * num_layers\n",
    "total_flops = lm_head_flops + all_block_flops\n",
    "\n",
    "embedding_weight_bytes = count_embedding_weights(hidden_size, vocab_size) * a16_ratio\n",
    "lm_head_weight_bytes = count_lm_head_weights(hidden_size, vocab_size) * ratio\n",
    "block_mm_weight_bytes = count_block_mm_weights(hidden_size, inter_size, num_attn_heads, num_kv_heads) * ratio\n",
    "block_gather_weight_bytes = count_block_gather_weights(seq_len, hidden_size, num_attn_heads) * a16_ratio\n",
    "mm_weight_bytes = lm_head_weight_bytes + block_mm_weight_bytes * num_layers\n",
    "\n",
    "kv_cache_bytes = count_kv_cache(seq_len, hidden_size, num_layers, num_attn_heads, num_kv_heads) * a16_ratio\n",
    "\n",
    "prefill_compute_time = get_prefill_compute_time(total_flops, num_device, tpu_freq)\n",
    "prefill_allreduce_time = get_prefill_allreduce_time(seq_len, hidden_size, num_layers, num_device, tpu_freq)\n",
    "decode_pcie_time = get_decode_pcie_time(num_layers)\n",
    "decode_allreduce_time = get_decode_allreduce_time(num_layers, num_device)\n",
    "decode_load_weights_time = get_decode_load_weights_time(mm_weight_bytes, num_device, tpu_freq)\n",
    "decode_load_kv_cache_time = get_decode_load_kv_cache_time(kv_cache_bytes, num_device, tpu_freq)\n",
    "total_prefill_time = prefill_compute_time + prefill_allreduce_time\n",
    "total_decode_time = decode_pcie_time + decode_allreduce_time + decode_load_weights_time + decode_load_kv_cache_time\n",
    "tps = 1000 / total_decode_time\n",
    "\n",
    "print(f'## Yi-34B-W4A16-seq{seq_len}-{num_device}dev-{tpu_freq}MHz:')\n",
    "print(f'### Basic Information')\n",
    "print(f'embedding: {embedding_weight_bytes/2**20} MiB')\n",
    "print(f'block_gather: {block_gather_weight_bytes/2**20} MiB')\n",
    "print(f'lm_head: {lm_head_flops/1e9} GFLOPs, {lm_head_weight_bytes/2**20} MiB')\n",
    "print(f'blocks: {block_flops/1e9} GFLOPs, {block_mm_weight_bytes/2**20} MiB')\n",
    "print(f'all blocks: {all_block_flops/1e9} GFLOPs, {block_mm_weight_bytes*num_layers/2**20} MiB')\n",
    "print(f'mm_weights: {mm_weight_bytes/2**20} MiB')\n",
    "print(f'total flops: {total_flops/1e9} GFLOPs')\n",
    "print(f'### Time Information')\n",
    "print(f'prefill_time = {total_prefill_time} s')\n",
    "print(f'decode_time = {total_decode_time} ms, Speed = {tps} token/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
